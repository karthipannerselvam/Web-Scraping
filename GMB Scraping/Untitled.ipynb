{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "080d2cb5",
   "metadata": {},
   "source": [
    "# GMB Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d011295b",
   "metadata": {},
   "source": [
    "Upgrading pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6990c21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\hp\\anaconda3\\lib\\site-packages (24.2)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddac7e6",
   "metadata": {},
   "source": [
    "Installing selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1b8d2e1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.14)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (0.26.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (2024.6.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d347d",
   "metadata": {},
   "source": [
    "Import necessary libraries\n",
    "time-To introduce delays in the script\n",
    "pandas-For data manipulation and storage\n",
    "selenium - For web browser automation\n",
    "json - For working with json data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f7edcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35956f7e",
   "metadata": {},
   "source": [
    "Define the URL for Google Maps and the path to the Chrome driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ec8971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "url = 'https://www.google.com/maps'\n",
    "path = r'C:\\Users\\hp\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe'\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.headless = True\n",
    "chrome_options.add_argument('--headless')\n",
    "service = Service(path)\n",
    "browser = webdriver.Chrome(service=service, options=chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf1f110",
   "metadata": {},
   "source": [
    "Function to search for escape rooms in a given location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f0c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_escape_rooms(location):\n",
    "    browser.get(url)\n",
    "    place = browser.find_element(By.XPATH, '//*[@id=\"searchboxinput\"]')\n",
    "    place.clear()\n",
    "    place.send_keys(f'escape rooms in {location}')\n",
    "    submit = browser.find_element(By.XPATH, '//*[@id=\"searchbox-searchbutton\"]')\n",
    "    submit.click()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c08f27b",
   "metadata": {},
   "source": [
    "Function to scrape links to individual escape room pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd2ceac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links():\n",
    "    links = set()\n",
    "    has_more_places = True\n",
    "\n",
    "    while has_more_places:\n",
    "        soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "        places = browser.find_elements(By.CLASS_NAME, 'hfpxzc')\n",
    "\n",
    "        if not places:\n",
    "            break\n",
    "\n",
    "        for place in places:\n",
    "            link = place.get_attribute('href')\n",
    "            if link:\n",
    "                links.add(link)\n",
    "\n",
    "        # Scroll down to load more places\n",
    "        prev_length = len(places)\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(4)\n",
    "        places = browser.find_elements(By.CLASS_NAME, 'hfpxzc')\n",
    "\n",
    "        if len(places) == prev_length:\n",
    "            has_more_places = False\n",
    "\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ecd585",
   "metadata": {},
   "source": [
    "Function to scrape links to individual escape room pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaebae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_gmb_data(links):\n",
    "    data = []\n",
    "    cnt=0\n",
    "\n",
    "    for link in links:\n",
    "        browser.get(link)\n",
    "        time.sleep(5)\n",
    "        soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            name = soup.find('h1', class_='DUwDvf lfPIob').text if soup.find('h1', class_='DUwDvf lfPIob') else ''\n",
    "            address = soup.find('div', class_='Io6YTe fontBodyMedium kR99db').text if soup.find('div', class_='Io6YTe fontBodyMedium kR99db') else 'Address not found'\n",
    "            phone_elem = soup.find('button', {'data-tooltip': 'Copy phone number'})\n",
    "            phone = phone_elem.find('div', class_='Io6YTe fontBodyMedium kR99db').text if phone_elem else 'Phone number not found'\n",
    "            url_elem = soup.find('a', {'data-tooltip': 'Open website'})\n",
    "            url = url_elem.get('href') if url_elem else 'url not found'\n",
    "            hours_div = soup.find('span', class_='ZDu9vd')\n",
    "            hours = ''\n",
    "            if hours_div:\n",
    "                spans = hours_div.find_all('span')\n",
    "                if spans:\n",
    "                    hours = spans[-1].text.strip()\n",
    "\n",
    "            review_elem = soup.find('button', {'class': 'HHrUdb fontTitleSmall rqjGif'})\n",
    "            reviews_text = 'review not found'\n",
    "            if review_elem:\n",
    "                spans = review_elem.find('span')\n",
    "                if spans:\n",
    "                    reviews_text = spans.text.strip()\n",
    "\n",
    "            data.append({\n",
    "                'name': name,\n",
    "                'address': address,\n",
    "                'phone number': phone,\n",
    "                'url': url,\n",
    "                'hours': hours,\n",
    "                'reviews': reviews_text,\n",
    "                'links': link\n",
    "            })\n",
    "\n",
    "            cnt += 1\n",
    "            print(f\"Processed {cnt}/{len(links)}: {name}\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting data: {e}\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79cd3a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations to scrape\n",
    "locations = [\"banglore\", \"chennai\", \"delhi\",\"pune\",\"mumbai\",\"kolkata\",\"hydrabad\",\"new york\",\"San Francisco\",\"ahmedabad\",\"atlanta\",\"kerala\",\"Pennsylvania\",\"New Jersey\",\"los angles\",\"texas\",\"london\",\"chicago\",\"las vegas\",\"san diego\",\"orland\",\"jaipur\",\"mysore\",\"coimbatore\",\"Australia\",\"Manchester\",\"Massachusetts\",\"Colorado\"]\n",
    "all_data = [] # List to store data from all locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7892ba47",
   "metadata": {},
   "source": [
    "Loop through each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f1096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/5: Mystery Rooms - Indira Nagar, Bangalore (OFFICIAL Escape Rooms)\n",
      "Processed 2/5: ESCAPE ROOM Koramangala (Previously Mystery junkies)\n",
      "Processed 3/5: Breakout® Escape Rooms | Koramangala | #1 Escape room in Bangalore\n",
      "Processed 4/5: The Amazing Escape - Escape Room Adventure\n",
      "Processed 5/5: Mystery Rooms - Whitefield, Bangalore (Escape Rooms with Live Actor)\n",
      "Processed 1/4: Breakthru - The Real Escape Room\n",
      "Processed 2/4: Freeing India\n",
      "Processed 3/4: Freeing India\n",
      "Processed 4/4: Mystery Rooms Chennai - OFFICIAL Escape Rooms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m search_escape_rooms(location)\n\u001b[0;32m      3\u001b[0m links \u001b[38;5;241m=\u001b[39m scrape_links()\n\u001b[1;32m----> 4\u001b[0m location_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_gmb_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m all_data\u001b[38;5;241m.\u001b[39mextend(location_data)\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mscrape_gmb_data\u001b[1;34m(links)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m links:\n\u001b[0;32m      6\u001b[0m     browser\u001b[38;5;241m.\u001b[39mget(link)\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(browser\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for location in locations:\n",
    "    search_escape_rooms(location)\n",
    "    links = scrape_links()\n",
    "    location_data = scrape_gmb_data(links)\n",
    "    all_data.extend(location_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde64c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the DataFrame to a CSV file\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('gmb_data.csv', index=True)\n",
    "\n",
    "# Save to JSON\n",
    "with open('gmb_data.json', 'w') as f:\n",
    "    json.dump(all_data, f, indent=4)\n",
    "\n",
    "print(\"Data saved to gmb_data.csv and gmb_data.json\")\n",
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
